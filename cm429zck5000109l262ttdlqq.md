---
title: "Links Discovery"
seoTitle: "Outils de découverte de liens: Katana, GoSpider et Hakrawler"
seoDescription: "Découvrez comment explorer les liens d'un site web avec Katana, GoSpider et Hakrawler pour identifier sa structure et ses ressources cachées"
datePublished: Fri Nov 29 2024 05:00:37 GMT+0000 (Coordinated Universal Time)
cuid: cm429zck5000109l262ttdlqq
slug: links-discovery
tags: cybersecurity, cybersecurity-1, bugbounty, bugbountytips, katana, gospider, hakrawler, linksdiscovery

---

Le Links Discovery ou la découverte de liens est un processus qui consiste à explorer et à extraire des liens et des ressources d’un site web pour obtenir des informations sur une cible. Il permet d’identifier la structure des pages et les relations entre elles pour mieux comprendre comment un site est construit. On peut aussi y ressortir les sous-domaines et d’autres ressources cachées ou moins visibles.

## Katana

Katana est un outil d’exploration de sites web, écrit en Go, dévéloppé par [**Project Discovery**](https://projectdiscovery.io/). Il permet aussi d’analyser des sites web, d’y extraire des ressources cachées et identifer des vulnérabilitées potentielles.

La documentation officielle se retrouve sur [**GitHub**](https://github.com/projectdiscovery/katana).

Pour installer Katana, utilisez la commande suivante:

```python
go install github.com/projectdiscovery/katana/cmd/katana@latest
```

### Options

* `katana -h` : cette option liste toutes les options de katana
    
* `katana -u [site.com] -d [INT] -e "STRING" -rl [INT] -o [results.json]`
    
    * l’option `-u` définit le site ou l’URL à analyser
        
    * l’option `-d` définit la profondeur d’exploration à partir du lien spécifié
        
    * l’option `-e` exclut divers éléments correspondants à la chaîne de caractère spécifiée. Cela peut être des chemins, des adresses IP, des sous-domaines etc.
        
    * l’option `-rl` fixe un nombre maximal de requêtes par secondes (150 par défaut)
        
* `cat file.txt | katana -jc -noi`
    
    * `cat` permet de lire un fichier qui contient une liste d’URLs (une par ligne) à explorer
        
    * l’option `-jc` active l’analyse des fichiers JavaScript
        
    * l’option `-noi` pour no incognito désactive le mode incognito, ce qui permet de conserver l'historique de navigation pendant l'exploration
        

Toute la documentation et les options sont disponibles sur [**Github**](https://github.com/projectdiscovery/katana?tab=readme-ov-file#running-katana).

La photo ci-dessous montre une exécution de l’outil. Elle explorera le site `http://youtube.com`, à une profondeur de 3, exclura les chemins contenant `/login`, active l’analyse des fichiers JavaScript, et enregistrera les résultats dans un fichier.

On voit donc que des liens et fichiers JavaScript sont bien extraits.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1731955773836/5ddf7f29-40c0-4352-87e4-1e7f48f49b29.png align="left")

  

## GoSpider

GoSpider est un outil de reconnaissance écrit en lang[age Go](https://github.com/projectdiscovery/katana?tab=readme-ov-file#running-katana), pour crawler ou extraire des données d’un site Web. Un crawler ou robot d’exploration est un programme ou un script automatisé destiné à collecter des informations sur des sites internet. GoSpider peut être utilisé lors de la reconnaissance pour identifier des liens, des ressources, des fichiers en lien avec un site spécifique.

La documentation officielle se retrouve sur [**GitHub**.](https://github.com/jaeles-project/gospider)

Pour installer GoSpider, utilisez la commande suivante:

```python
go get -u github.com/jaeles-project/gospider
```

### Options

* `gospider -h` : l’option `-h` permet de voir la liste de toute les options de GoSpider
    
* `gospider -s [site.com/URL] -c [INT] -o [resultat.txt/.json]`
    
    * l’option `-s` spécifie l’URL ou le site à analyser;
        
    * l’option `-c` définit le nombre maximal de requêtes simultanées lors de l’exploration du site (par défaut, il est fixé à 5);
        
    * l’option `-o` enregistre les résultats dans un fichier;
        
* `gospider -S [sites] -q --blacklist [ ] -d [INT]`
    
    * l’option `-S` spécifie une fichier texte contenant les URLS de sites à analyser;
        
    * l’option `-q` rend l’analyse plus concise et désactive les logs détaillés;
        
    * l’option `--blacklist` permet d’exclure (blacklister) soit des extensions, des types de fichiers lors de l’analyse.
        
    * l’option `-d` définit la profondeur de lien à explorer lors du crawl. Cela signique qu’il explore d’abord l’URL initiale, ensuite les liens trouvés sur cette page, et ensuite les liens sur ces pages, et ainsi de suite. Il est fixé à 1 par défaut.
        

En parcourant la liste des options, vous aurez accès aux différentes manières d’utiliser GoSpider.

## Hakrawler

Hakrawler est aussi un outil de reconnaissance utilisé pour explorer des sites web et extraire des informations comme des liens, des formulaires et des emplacements de fichiers . Aussi écrit en langage Go, il est utilisé lors de la phase de reconnaissance lors des tests de sécurité.  
La documentation officielle est disponible sur [**GitHub**](https://github.com/hakluke/hakrawler).

Pour installer Hakrawler, utilisez la commande suivante:

```python
go get -u github.com/hakluke/hakrawler
```

### Options

* `hakrawler -h` : cette option affiche toutes les options de hackrawler
    
* `echo [URL] | hakrawler -t [INT] -d [INT] -t [INT] | grep "STRING"`
    
    * la commande `echo` envoie l’URL en tant qu’entrée à Hakrawler via le pipe `|`
        
    * l’option `-t` pour threads, définit le nombre de requêtes simultanées pour accélerer l’exploration. Cela permet de crawler plusieurs pages à la fois
        
    * l’option `-d` définit la profondeur du crawl
        
    * l’option `-t` fixe le délai dattente pour chaque requête. Passé ce délai, la requête d’exploration est abandonnée si elle ne recoit aucune réponse
        
    * l’option `grep` filtre les résultats du hakrawler selon le string défini
        
* `cat sites.txt | hakrawler -subs -json`
    
    * le contenu du fichier “sites.txt” est lu (une URL par ligne) et redirigé à Hakrawler via le symbole pipe `|`
        
    * l’option `-subs` indique qu’il faut analyser aussi les sous-domaines liés à chaque url dans le fichier
        
    * l’option `-json` retourne les résultats en format JSON
        

La photo ci-dessous montre une exécution de l’outil. La commande explore le site avec une profondeur d'exploration de 2 niveaux, incluant les sous-domaines et affiche les résultats en format JSON

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1731956445243/c55b4f64-2543-4466-97f6-778f3b698333.png align="center")

Ces outils sont complémentaires dans l’exploration et l'analyse de sites web dans le cadre de la reconnaissance. Ils permettent de découvrir des liens, des fichiers et des ressources cachées qui peuvent être utiles pour identifier des vulnérabilités

<mark>À noter, qu’il est important de rappeler que l'utilisation de ces outils doit toujours se faire dans un cadre légal.</mark>